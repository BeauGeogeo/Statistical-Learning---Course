{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55af40d9",
   "metadata": {},
   "source": [
    "# <center> Apprentissage statistique <center>\n",
    "## <center> Projet <center>\n",
    "### <center> Bounds on the prediction error of penalized least squares estimators with convex penalty <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(0)\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_informative = 10\n",
    "bias = 1\n",
    "noise = 2\n",
    "\n",
    "dataset = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative, bias=bias, noise=noise, \n",
    "                          coef=True, random_state=0)\n",
    "\n",
    "dataset, coef = dataset[:2], dataset[2]\n",
    "\n",
    "# set the parameters of the linear model with the values of those used by sklearn to create the dataset\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.coef_ = coef\n",
    "lr.intercept_ = bias\n",
    "\n",
    "f = lr.predict(dataset[0])  # the mean vector, that we can compute thanks to the coef given by make_regression\n",
    "\n",
    "added_noise = dataset[1] - f  # noise added by sklearn function make_regression, i.e y - f with the paper notations\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(*dataset, test_size=0.2, train_size=0.8, random_state=0, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f856a8",
   "metadata": {},
   "source": [
    "## Ordinary least square estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the restriction of f on the test set (needed because it has been shuffled by train_test_split)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.coef_ = coef\n",
    "lr.intercept_ = bias\n",
    "\n",
    "f_test = lr.predict(X_test)\n",
    "\n",
    "# fit the model and make predictions\n",
    "\n",
    "lr.fit(X_train, y_train) # Note that fitting again the model erase the previous weights\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# compute the error (MSE) between our predictor and the vector f\n",
    "\n",
    "error = MSE(f_test, predictions) \n",
    "\n",
    "print(f\"MSE for OLS regression {error}\\n\")\n",
    "print(f\"l2-norm of the weights for OLS model : {np.linalg.norm(lr.coef_)}\")\n",
    "print(f\"l1-norm of the weights for OLS model : {np.linalg.norm(lr.coef_, ord=1)}\")\n",
    "\n",
    "print(f\"\\n\\tMeasure the spreadness inside the OLS vector of weights\")\n",
    "\n",
    "coeff_OLS = pd.Series(lr.coef_)\n",
    "coeff_OLS.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c61fc",
   "metadata": {},
   "source": [
    "## Lasso estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52663d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we fit the lasso estimator with different alpha parameter (plays the role of lambda here) and see if it performs better\n",
    "\n",
    "all_alphas = np.arange(10) / 100\n",
    "\n",
    "for alpha in all_alphas:\n",
    "\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    predictions = lasso.predict(X_test)\n",
    "\n",
    "    error = MSE(f_test, predictions) \n",
    "    \n",
    "    if alpha == 0.1:\n",
    "        \n",
    "        print(\"\\t\\t Measure the spreadness inside the Lasso vector of weights\\n\")\n",
    "        coeff_lasso = pd.Series(lasso.coef_)\n",
    "        print(np.where(lasso.coef_ == np.max(lasso.coef_)))\n",
    "        print(coeff_lasso.describe())\n",
    "        print()\n",
    "    \n",
    "    print(f\"MSE of Lasso : {error} with alpha = {alpha}\")\n",
    "    print(f\"l2-norm of the weights for Lasso model : {np.linalg.norm(lasso.coef_)}\")\n",
    "    print(f\"l1-norm of the weights for OLS model : {np.linalg.norm(lasso.coef_, ord=1)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad4e05",
   "metadata": {},
   "source": [
    "With a sample of n = 10000\n",
    "\n",
    "With 10000 features :\n",
    "\n",
    "With only 10 informative features :\n",
    "\n",
    "This is a classical case were simple linear regression will perform bad. There are so few informative features, OLS will catch a lot of noise and is prompt to overfitting. It has a MSE of 6205.81 and the l2-norm of the weights is 160.32.\n",
    "With $\\alpha = 0.1$, lasso is so much better with a loss of only 0.08. It can be 1 or 2 order of magnitude bigger when $\\alpha$ is greater, but it is still far below the OLS MSE. \n",
    "Curiously, the weights norm is bigger (6046.88) for the lasso than for the OLS. Probabily the Lasso was able to select the usefull variables and putting big weights on them was still achieving better results and a better optimization of the regularized cost function than what the OLS was able to achieve on the original cost function. \n",
    "This last intuition is in fact confirmed when one prints the l1-norm, and it totally makes sense. It is huge for OLS (6046.88), but for the lasso with $\\alpha = 0.1$, it is much lower (478.13), even if the l2-norm of the weights is really close to the l2-norm of OLS weights. It can only be the case if many coefficients of the lasso are shrinked towards zero, such that taking the square makes them even smaller. So even if some weights of the lasso are still big, the others are made so close to 0 with the l2-norm that it compensates. OLS have certainly more spreaded weights, so when we use l2-norm, the important shrinkage induced by the square mitigate the difference between each estimator's weights, so their l2-norm is pretty close (if the weights are close to zero). For example, $1^2 - 0.05^2$ is higher than $0.8^2 - 0.3^2$ that in turn is higher than $0.6^2 - 0.1^2$.\n",
    "But with l1-norm, the difference is stronger, we have not the \"non linear\" effect of the square that will shrink many of the weights close to zero even closer, reducing the difference between the estimator's weights that are closed to zero. l1-norm on OLS will add many positive quantities with now a bigger difference with the lasso weights, such that at the end, OLS l1-norm is particularly higher than Lasso, and now choosing this norm we better understand the regularization effects of the latter algorithm. It is also consistent with the fact that $\\forall x \\in \\mathbb{R}^d, \\lVert x \\rVert_2 \\leq \\lVert x \\rVert_1$.\n",
    "\n",
    "Wa can also notice that the norm of the weights increases when $\\alpha$ ($\\lambda$ in the paper) increases, which is logical as bigger lambda means stronger penalization of the cost function. But it does not necessarily mean that the model becomes better and better. In many of our experiments, the best value for $\\alpha$, regarding a range between 0 and 1 with 0.1 steps, is very often 0.1. So a bit of regularization is good, but too much can lead to a slight decrease of performances. This is the classical Bias-Variance Tradeoff of machine learning and statistics. We have to find the best balance between unerfitting and overfitting. Another fact that can explain a low value for $\\alpha$ is the setting. Here, when you have so much features and hence a consequent number of weights, each one is multiplied by $\\alpha$, such that increasing $\\alpha$ lead to a greater value than if you had less weights. Imagine you have 10 weights, each equal to 1. Then 0.1 * 10 = 1, and 0.2 * 10 = 2, and 2 - 1 = 1. Now if all weights are still constant but now they are 10000, we now have respectively 1000, 2000, and a significative difference of 1000.\n",
    "\n",
    "When we print the spreadness of the parameters vectors, it confirms our intuition. There are only few non-zero weights for the Lasso, such that the l2-norm of some big weights is close to the l2-norm of more non-zero weights a bit smaller, concavity of the square root clearly playing a role here. But now, if we only add the coordinates of the parameters vectors, the sparsity of the lasso vector yields to better results and much smaller norm than the OLS.\n",
    "\n",
    "With 100 informative features :\n",
    "\n",
    "Roughly the same.\n",
    "\n",
    "With 1000 informative features :\n",
    "\n",
    "Same.\n",
    "\n",
    "\n",
    "With n_features = 1000\n",
    "\n",
    "With 10 informative features :\n",
    "\n",
    "OLS now performs better with 0.55 MSE, but Lasso is still better with 0.08 MSE at $\\alpha = 0.1$. For the following value of $\\alpha$, it is still better but after regularization is too strong and Lasso is worst than OLS.\n",
    "\n",
    "So even with a small ratio of informative features / number of features, OLS is not horrible and not too far from Lasso. As there are only ten significant features, it is still logical that Lasso performs better with appropriate $\\alpha$, but this advantage now very rapidly vanishes with $\\alpha$ increasing. It is surely linked to the fact that with less features, even if the informative ones are not numerous, we have less risks of multicolinearity and also less risks for the OLS to overfit by using all the different features.\n",
    "\n",
    "With 100 informative features :\n",
    "\n",
    "Now MSE of OLS is a bit higher than the MSE of the Lasso. Again, it matches our understanding. As we have more informative features, the less we need to shrink some weights beacause of uninformative features. Hence wee need to have more weights that are non-zeros, and then to explore with more freedom the parameters space. The regularization is probabiliy to strong and prevent the lasso model to learn the usefull weights. To confirm this hypothesis, let's run the code again but with smaller values of $\\alpha$, i.e with a smaller penalty. \n",
    "It is exactly what happens, the weights get bigger and now the MSE of Lasso is again less than for the OLS. We clearly see the tradeoff mechanism. Moreover, a fine-tuning of lambda (we're doing a kind of cross-validation here with our grid search) can always lead to an acceptable regularization. \n",
    "\n",
    "\n",
    "Note that with $\\alpha = 0$ we have no regularization, so logically we get the same error. Sometimes, Lasso performs even worse than OLS. It can happen and it's not for a mathematical reason but an implementation reason. Sklearn warns us that even if it should be the same, $\\alpha = 0$ is discouraged in LASSO because the implementation of the algorithm can then lead to far worse results and could fail to converge, as it is the case here."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9867cebb",
   "metadata": {},
   "source": [
    "A remark about comparing the norm of the weights.\n",
    "\n",
    "First and foremost, note that we could have chosen to take only the l2-norm to compare the weights of OLS and Lasso. Indeed, the fact that weights have been tuned in the lasso using the l1-norm is irrelevant to which norm we should use to compare the two estimators. Whatever the norm, it will still give us a way to compare the weights \"size\" of each model. \n",
    "\n",
    "Especially when we are in finite dimension. Because of the equivalence of the norms, we should not worry too much about which one is chosen to make the comparison.   \n",
    "\n",
    "But in reality, as we have just seen, there is a practical reason to choose a different norm than l2 and choosing the l1-norm, which is the norm in the Lasso. The difference in the weights magnitude between Lasso and OLS is much more emphasized. It makes sense as the choice of l1-norm gives a shrinkage of some coefficients to zero while others can stay high, which then lead to a great difference in l1-norm with OLS as seen above. An interesting thing to do should be to compare with ridge regression that uses l2-norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lipschitz(dico_estim_y):\n",
    "    \n",
    "    differences_y = []\n",
    "    differences_estimators = []\n",
    "    len_dico = len(dico_estim_y)\n",
    "\n",
    "    for i in range(len_dico):\n",
    "        \n",
    "        yi = dico_estim_y[i][0] # we note y_i the ith vector of the dictionary but it's a vector of R^d, \n",
    "        # not necessarily a single coordinate  in general\n",
    "        estimator_yi = dico_estim_y[i][1] # the coresponding estimator X Beta_hat(yi)\n",
    "        \n",
    "        for j in range(i + 1, len_dico):\n",
    "            \n",
    "            yj = dico_estim_y[j][0]\n",
    "            estimator_yj = dico_estim_y[j][1]\n",
    "            differences_y.append(MSE(yi, yj, squared=False))  # set squared = False to have the norm as in the paper, and not the norm squared\n",
    "            differences_estimators.append(MSE(estimator_yi, estimator_yj, squared=False))\n",
    "            \n",
    "    return differences_y, differences_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b71e38-a0d7-42a7-b792-a24d418e1c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now verify that the Lipschitz character for the estimator holds with L = 1\n",
    "\n",
    "rng = default_rng(0)\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_informative = 10\n",
    "bias = 1\n",
    "\n",
    "noise = 2 # fixed noise now\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "dico_estim_y = {}\n",
    "\n",
    "nb_realizations = 10\n",
    "\n",
    "f = None\n",
    "\n",
    "dico_estim_y = {}\n",
    "\n",
    "for i in range(nb_realizations):\n",
    "    \n",
    "    if not i:\n",
    "        \n",
    "        dataset = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative, bias=bias, noise=noise, \n",
    "                              coef=True, random_state = 0)\n",
    "\n",
    "        dataset, coef = dataset[:2], dataset[2]\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        lr.coef_ = coef\n",
    "        lr.intercept_ = bias\n",
    "\n",
    "        f = lr.predict(dataset[0])  # the mean vector, that we can compute thanks to the coef given by make_regression\n",
    "\n",
    "        lasso.fit(dataset[0], dataset[1])\n",
    "\n",
    "        predictions_lasso = lasso.predict(dataset[0])\n",
    "\n",
    "        dico_estim_y[i] = [dataset[1], predictions_lasso]\n",
    "        continue\n",
    "        \n",
    "    new_y = f + rng.normal(0, noise, n_samples)\n",
    "    \n",
    "    lasso.fit(dataset[0], new_y)\n",
    "   \n",
    "    predictions_lasso = lasso.predict(dataset[0])\n",
    "\n",
    "    dico_estim_y[i] = [new_y, predictions_lasso]\n",
    "    \n",
    "\n",
    "differences_y, differences_estimators = compute_lipschitz(dico_estim_y)\n",
    "axis = np.arange(len(differences_y))\n",
    "\n",
    "plt.plot(axis, differences_y, label='diff y, y\\'', color='tab:orange')\n",
    "plt.plot(axis, differences_estimators, label='diff estimator_y, estimator_y\\'', color='b')\n",
    "plt.legend()\n",
    "plt.title(\"The Lipschitz condition holds.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93ecac93",
   "metadata": {},
   "source": [
    "The 1-Lipschitz property hold. A way to be sure that it is indeed the case is to check it numerically. It's done in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences_estimators = np.array(differences_estimators)\n",
    "differences_y = np.array(differences_y)\n",
    "\n",
    "np.sum(differences_estimators > differences_y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00b7862c",
   "metadata": {},
   "source": [
    "The sum of the boolean equal to true when the bound is violated is 0, meaning it never happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb956a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(differences_estimators == differences_y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d2dea54",
   "metadata": {},
   "source": [
    "No case of ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(differences_estimators < differences_y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86f7e345-2225-4800-8ef2-f75a3c4270d1",
   "metadata": {},
   "source": [
    "All the quantities we are interested in are strictly inferior to their lipschitz bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073e8b5-d34f-4c15-af52-d46c3a0da449",
   "metadata": {},
   "source": [
    "We are now going to illustrate **Theorem 3.2**. We will first generate different noises to obtain different vectors \\mathbf{y}, and find a value $R$ for which we have the condition about the probability in this theorem that holds. Of course, a theoretical value for R is given in the paper, but it is linked to oracle inequalities and we are not sure if such a value of R is easy and fast to compute. So we decided for the moment to follow a less ambitious but safer path.\n",
    "\n",
    "Once we have our $R$, we'll check that our results are consistent with the boundaries mentionned in the theorem. Be aware that we will compute an approximation of the probability involved, we don'use an exact expression, instead we approximate with an arbitrary (not too low) number of realizations of the gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb0811-4f3e-4718-8eca-fe9ef7e5aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(0)\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_informative = 10\n",
    "bias = 1\n",
    "\n",
    "noise = 2 # fixed noise now\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "nb_realizations = 20\n",
    "\n",
    "f = None\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(nb_realizations):\n",
    "    \n",
    "    if not i:\n",
    "\n",
    "        dataset = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative, bias=bias, noise=noise, \n",
    "                              coef=True, random_state = 0)\n",
    "    \n",
    "        dataset, coef = dataset[:2], dataset[2]\n",
    "        \n",
    "        lr = LinearRegression()\n",
    "        lr.coef_ = coef\n",
    "        lr.intercept_ = bias\n",
    "    \n",
    "        f = lr.predict(dataset[0])  # the mean vector, that we can compute thanks to the coef given by make_regression\n",
    "        \n",
    "        lasso.fit(*dataset)\n",
    "    \n",
    "        predictions_lasso = lasso.predict(dataset[0])\n",
    "        all_predictions.append(predictions_lasso)\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    new_noise = rng.normal(0, 2, n_samples)\n",
    "    \n",
    "    new_y = f + new_noise\n",
    "    \n",
    "    lasso.fit(dataset[0], new_y)\n",
    "\n",
    "    predictions_lasso = lasso.predict(dataset[0])\n",
    "    all_predictions.append(predictions_lasso)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_pred_errors = np.array([MSE(prediction, f, squared=False) for prediction in all_predictions])\n",
    "\n",
    "# then, thanks to pandas, it's easy to have the median and so our R\n",
    "\n",
    "print(f\"Number of prediction errors computed : {all_pred_errors.shape[0]}\\n\")\n",
    "median = pd.Series(all_pred_errors).median()\n",
    "\n",
    "print(f\"Median of these predictions errors (which gonna be our R) {median}\\n\")\n",
    "print(f\"We check the number of prediction errors <= median to see if it's correct : {np.sum(all_pred_errors <= median)}.\\nIt's correct !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5d2a-97fa-43cc-a38f-3949d434cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before doing the computation, we need to write a function to compute the \"empirical\" probability, i.e ecdf\n",
    "\n",
    "def compute_emp_proba(variable_values, x_values):  # return empirical probability of being inferior to some quantity x\n",
    "    return np.mean(variable_values.reshape(variable_values.shape[0], 1) <= x_values, axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1c303-ef01-4511-81eb-228103ff499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we verify the given bound\n",
    "\n",
    "R = median\n",
    "\n",
    "t_max = (np.max(all_pred_errors) - R) * np.sqrt(n_samples) / noise  # we create meaningful values for t to well illustrate the bound\n",
    "eps = 10e-8  # because t should not be zero, t > 0 \n",
    "range_t = np.linspace(eps, t_max, 100)\n",
    "x_values = R + range_t * noise / np.sqrt(n_samples)  # * noise because it is the std here\n",
    "\n",
    "emp_probas = compute_emp_proba(all_pred_errors, x_values) # have \n",
    "\n",
    "plt.plot(range_t, norm.cdf(range_t, loc=0, scale=1), label=\"cdf N(0, 1)\")\n",
    "plt.plot(range_t, emp_probas, label=\"P(prediction error)\")\n",
    "plt.legend()\n",
    "plt.title(f\"First inequality of Theorem 3.2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952b3e-54de-4cfe-b2e6-3c9dc50a57cb",
   "metadata": {},
   "source": [
    "Again we see that the inferior bound always hold, except in a very tiny interval at the beginning but this perfectly makes sense since we are using the empirical cumulative distribution function. So when we increase R, i.e the median here, by a very small amount (a very little t), it does not change anything, whereas for the cumulative distribution function it changes, it indeed increases. (Sure the cdf in the specialized library we used is also discretized, everything is at the end in computer science, but the discretization granularity must be much higher).  \n",
    "\n",
    "In the cell below, we see that the bound for the expectation is also verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378eaaaa-7a03-4c3b-8ff7-be87b59088d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_error = np.mean(all_pred_errors)\n",
    "print(f\"Approximation of the expectation : {approx_error} <= {R + noise / np.sqrt(2 * np.pi * n_samples)} the bound given in the paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198465a3-4cee-4ab4-ba12-73674988d78f",
   "metadata": {},
   "source": [
    "We just verified that the bounds of **Theorem 3.2** perfectly hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5ae6c-4fa5-4522-afe8-3d162fc16bac",
   "metadata": {},
   "source": [
    "As the theoretical bounds for oracle inequalities imply solving several constrained optimization problems, we kind ran out of time to illustrate this properly. Nevertheless, we let the snippet of code used to check condition on the event that plays a major role in all the section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5503b-2f1d-4fc7-88d2-0c79fea6e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dataset[0].shape[1]\n",
    "original_noise = (dataset[1] - f)\n",
    "lambda_param = 2 * noise * np.sqrt(2 * np.log2(p) / n_samples)\n",
    "norm_max = (1 / n_samples) * np.max(dataset[0].T @ original_noise)\n",
    "lambda_param / 2\n",
    "\n",
    "print(f\"Event of proposition 4.1 We check that the quantity in the left is inferior to lambda/2.\\n{norm_max} <= {lambda_param / 2}. It's ok !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
